# -*- coding: utf-8 -*-
"""PTQ-NLP-File_Cleaner

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H5XwVOwBvHfr6p9-gYP104wkz15T5yNX
"""

!pip install hazm==0.9.3

!pip install python-docx

from hazm import *
import os
from docx import Document
import pandas as pd
import re

# تابع باز کردن فایل ها با کمی تغییرات
def open_file(file_path):
    file_name, file_ext = os.path.splitext(file_path)

    if file_ext == '.doc' or file_ext == '.docx':
        document = Document(file_path)
        full_text = "\n".join([paragraph.text for paragraph in document.paragraphs])
        return full_text
    elif file_ext == '.csv':
        df = pd.read_csv(file_path, delimiter='\t', on_bad_lines='skip',encoding="utf-8", quoting=1)
        return df
    elif file_ext == '.dat':
        df = pd.read_csv(file_path,delimiter='\t', header=None)
        return df
    elif file_path.endswith('.txt'):
        with open(file_path, 'r') as file:
            data = file.read()
            return data
    else:
        print(f"Unsupported file format for: {file_path}")
        return None

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
import hazm
import string
import re

from hazm import POSTagger

modell = '/content/pos_tagger.model'

# فایل تگر ارسال شده
tagger = POSTagger (model = modell )

# این فانکشن برای تبدیل فاصله در افعال و جمع ها به نیم فاصله میباشد و برای ریشه یابی مناسب است
# از این فانکشن در تست آخر به دلیل ماهیت برنامه استفاده نشده
def convert_to_half_space(text):
  converted_text = re.sub(r'(?<=\S) (?=ها\b)', '‌', text)
  converted_text = re.sub(r'(?<=می) (?=\S)', '‌', converted_text)
  return converted_text

# تصحیح کلماتی نظیر عالللللیه که در آن نویسنده از شکل اشتباهی برای بیان احساسات استفاده کرده
def remove_consecutive_duplicates(text):
    result = ""
    previous_char = None

    for char in text:
        if char != previous_char:
            result += char
        previous_char = char

    return result

# تغییر جمله ی محاوره ی به شکل درست فعل+است
def modify_sentences(sentences):
    modified_sentences = []
    for sentence in sentences:
        words = sentence.split()
        if words[-1].endswith('ه'):
            words[-1] = words[-1][:-1]+ ' ' + 'است'
        modified_sentences.append(' '.join(words))
    return modified_sentences

# مثال
tokenized_sentences = ['این یک جمله خفنه', 'این یک جمله نیست.', 'این جمله ها قابل تغییر است']
modified_sentencesss = modify_sentences(tokenized_sentences)
modified_sentencesss

#جداسازی جملات بر اساس افعال
def verb_tokenizer(text):
    tagger = hazm.POSTagger(model=modell)
    tokens = hazm.word_tokenize(text)
    tagged_tokens = tagger.tag(tokens)

    # Filter out punctuation tokens
    filtered_tokens = [(token, tag) for token, tag in tagged_tokens if not re.findall(r'^[^\w]', token)]
    tokens = []
    current_token = ""

    for token, tag in filtered_tokens:
        if tag.startswith("V"):
            if current_token:
                tokens.append(current_token)
            tokens.append(token)
            current_token = ""
        else:
            if current_token:
                current_token += " " + token
            else:
                current_token = token

    if current_token:
        tokens.append(current_token)
    i = 1
    while i < len(tokens):
        if len(tokens[i].split()) == 1:
            tokens[i-1] += ' ' + tokens[i]
            tokens.pop(i)
        else:
            i += 1

    return tokens

# تصحیح فاصله ها
def fix_half_space(text):
    half_space = "\u200C"
    normal_space = " "

    fixed_text = text.replace(half_space, normal_space)
    return fixed_text

"""استاپ ورد ها"""

stop_words=open_file('/content/stopwords.dat')

stop_words.tail(5)

# تغییر لیست استاپ ورد ها در صورت نیاز زیرا در تست استفاده از تمامی کلات استاپ ورد تاثیر منفی در پردازش جمله داشت
#بسته به نیاز مسئله میتوان لیست اندیس هارا تغییر داد
indicies = [0,1,2,3,4,5,6,7,8,9,10,11,12,13,14,15,16,17,20,23,31,33,46,49,68,72,77,90,102,109,388]
final_stop_words = stop_words.loc[indicies]
final_stop_words=final_stop_words.reset_index(drop=True)

# تابع حذف استاپ ورد ها
def remove_stop_words(text):
    words = text.split()
    words = [word for word in words if word not in stop_words.iloc[:, 0].tolist()]
    return ' '.join(words)

text = "سلام و وقت بخیر . امروز می خواهیم درباره ی اثر کمونیسم بر یک جامعه و چگونگی تخریب جامعه  توسط آن ایدئولوژی صحبت بکنیم . امیدوارم این متن مورد قبول شما خوانندگان باشد "
print(text)
print(convert_to_half_space(text))
print(verb_tokenizer(text))

remove_consecutive_duplicates("عالللی بود همه چه درست و به اندازه و کیفیت خوب، امیداورم همیشه کیفیتتون خوب باشه ما مشتری همیشگی بشیممممممم")

fix_half_space('خیلی مسخرس یک ساعت باید منتظر موند و تازه ۳ هزار تومان هم پول اسنپ اکسپرس بدی در صورتی که رستورانای اطراف تلفنی با پیک‌مجانی میارن حالا اگه زود بیارید اشکالی نداره پول هم بگیرید بابتش ولی نه')

final_test = 'تاریخی‌ترین رویداد تصادفی که می‌توان به آن اشاره کرد، جنگ تروا در قرن ۱۲ قبل از میلاد است. این جنگ در نتیجهٔ احتکار یک زن زیبا به نام هلن توسط پاریس، پرنس تروا، از هم‌گسیختگی بین امپراطوری‌های یونانی‌تبار باشکوه آژانتینا وپارتنون شبه‌جزیرهٔ آتیکا رواج یافت. این جنگ، سال‌ها طول کشید و می‌توان آن را یکی از اصلی‌ترین علل نابودی امپراطوری‌های حضوریسیه و آزانسینی را در نظر گرفت. جالب است بدانید که این جنگ برای عاشقان تاریخ و اسطوره شناسان همواره موضوع جذابیت بالایی داشته است شاید دلیل آن را بتوان یافت در شگفتی‌هایی که دوران باستان به ما هدیه کرده است. در مورد جنگ تروا، منبعی برخی از پژوهش‌ها و داستان‌نویسان هستند که به شدیدترین منافع تک نظری علاقمندند. به هر حال، این موضوع موجب ابراز فرهنگ و هنر جههههههانننننن باستانی شده است و تا به امروزززززز برای ما دست خوش تمجیدها و نقدهایی حتی تا حدی قاااااابل قبول می‌ماند '

alpha = fix_half_space(final_test)
beta = remove_consecutive_duplicates(alpha)
gamma = verb_tokenizer(beta)
epsilon = modify_sentences(gamma)
epsilon